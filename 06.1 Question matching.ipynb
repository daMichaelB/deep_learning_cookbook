{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Questions from StackExchange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7z must be installed**\n",
    "\n",
    "https://gist.github.com/P7h/9fcccc54596ad05764128dec6f6cf78d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-09 10:12:24--  https://www.mirrorservice.org/sites/dl.fedoraproject.org/pub/epel/7/x86_64/Packages/p/p7zip-16.02-10.el7.x86_64.rpm\n",
      "Resolving www.mirrorservice.org (www.mirrorservice.org)... 212.219.56.184, 2001:630:341:12::184\n",
      "Connecting to www.mirrorservice.org (www.mirrorservice.org)|212.219.56.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 618060 (604K) [application/x-redhat-package-manager]\n",
      "Saving to: ‘p7zip-16.02-10.el7.x86_64.rpm’\n",
      "\n",
      "p7zip-16.02-10.el7. 100%[===================>] 603.57K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2019-08-09 10:12:24 (8.07 MB/s) - ‘p7zip-16.02-10.el7.x86_64.rpm’ saved [618060/618060]\n",
      "\n",
      "--2019-08-09 10:12:24--  https://www.mirrorservice.org/sites/dl.fedoraproject.org/pub/epel/7/x86_64/Packages/p/p7zip-plugins-16.02-10.el7.x86_64.rpm\n",
      "Resolving www.mirrorservice.org (www.mirrorservice.org)... 212.219.56.184, 2001:630:341:12::184\n",
      "Connecting to www.mirrorservice.org (www.mirrorservice.org)|212.219.56.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 990680 (967K) [application/x-redhat-package-manager]\n",
      "Saving to: ‘p7zip-plugins-16.02-10.el7.x86_64.rpm’\n",
      "\n",
      "p7zip-plugins-16.02 100%[===================>] 967.46K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2019-08-09 10:12:25 (11.4 MB/s) - ‘p7zip-plugins-16.02-10.el7.x86_64.rpm’ saved [990680/990680]\n",
      "\n",
      "warning: p7zip-16.02-10.el7.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 352c64e5: NOKEY\n",
      "warning: p7zip-plugins-16.02-10.el7.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 352c64e5: NOKEY\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.mirrorservice.org/sites/dl.fedoraproject.org/pub/epel/7/x86_64/Packages/p/p7zip-16.02-10.el7.x86_64.rpm\n",
    "!wget https://www.mirrorservice.org/sites/dl.fedoraproject.org/pub/epel/7/x86_64/Packages/p/p7zip-plugins-16.02-10.el7.x86_64.rpm\n",
    "\n",
    "!sudo rpm -U --quiet p7zip-16.02-10.el7.x86_64.rpm\n",
    "!sudo rpm -U --quiet p7zip-plugins-16.02-10.el7.x86_64.rpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers, models, utils\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_everything():\n",
    "    import tensorflow as tf\n",
    "    %reset -f in out dhist\n",
    "    tf.reset_default_graph()\n",
    "    K.set_session(tf.InteractiveSession())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for our networks.  We keep these deliberately small to reduce training time.\n",
    "\n",
    "VOCAB_SIZE = 250000\n",
    "EMBEDDING_SIZE = 100\n",
    "MAX_DOC_LEN = 128\n",
    "MIN_DOC_LEN = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from Stack Exchange is available by category (here we use \"travel\") and free to download.\n",
    "\n",
    "The downloaded file is .xml. We parse it and convert it to json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "99000/1000000"
     ]
    }
   ],
   "source": [
    "def extract_stackexchange(filename, limit=1000000):\n",
    "    json_file = filename + 'limit=%s.json' % limit\n",
    "\n",
    "    rows = []\n",
    "    for i, line in enumerate(os.popen('7z x -so \"%s\" Posts.xml' % filename)):\n",
    "        line = str(line)\n",
    "        if not line.startswith('  <row'):\n",
    "            continue\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print('\\r%05d/%05d' % (i, limit), end='', flush=True)\n",
    "\n",
    "        parts = line[6:-5].split('\"')\n",
    "        record = {}\n",
    "        for i in range(0, len(parts), 2):\n",
    "            k = parts[i].replace('=', '').strip()\n",
    "            v = parts[i+1].strip()\n",
    "            record[k] = v\n",
    "        rows.append(record)\n",
    "        \n",
    "        if len(rows) > limit:\n",
    "            break\n",
    "    \n",
    "    with open(json_file, 'w') as fout:\n",
    "        json.dump(rows, fout)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "xml_7z = utils.get_file(\n",
    "    fname='travel.stackexchange.com.7z',\n",
    "    origin='https://ia800107.us.archive.org/27/items/stackexchange/travel.stackexchange.com.7z',\n",
    ")\n",
    "print()\n",
    "\n",
    "rows = extract_stackexchange(xml_7z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99064,\n",
       " {'Id': '1',\n",
       "  'PostTypeId': '1',\n",
       "  'AcceptedAnswerId': '393',\n",
       "  'CreationDate': '2011-06-21T20:19:34.730',\n",
       "  'Score': '8',\n",
       "  'ViewCount': '467',\n",
       "  'Body': \"&lt;p&gt;My fiancée and I are looking for a good Caribbean cruise in October and were wondering which islands are best to see and which Cruise line to take?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems like a lot of the cruises don't run in this month due to Hurricane season so I'm looking for other good options.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; We'll be travelling in 2012.&lt;/p&gt;&#xA;\",\n",
       "  'OwnerUserId': '9',\n",
       "  'LastEditorUserId': '101',\n",
       "  'LastEditDate': '2011-12-28T21:36:43.910',\n",
       "  'LastActivityDate': '2012-05-24T14:52:14.760',\n",
       "  'Title': 'What are some Caribbean cruises for October?',\n",
       "  'Tags': '&lt;caribbean&gt;&lt;cruising&gt;&lt;vacations&gt;',\n",
       "  'AnswerCount': '4',\n",
       "  'CommentCount': '4',\n",
       "  'ClosedDate': '2013-02-25T23:52:47.953'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows), rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "Now that we have extracted our data, let's clean it up and take a look at what we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>Id</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>...</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Title</th>\n",
       "      <th>ViewCount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>393</td>\n",
       "      <td>4</td>\n",
       "      <td>&amp;lt;p&amp;gt;My fiancée and I are looking for a go...</td>\n",
       "      <td>2013-02-25T23:52:47.953</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:19:34.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-05-24T14:52:14.760</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>&amp;lt;caribbean&amp;gt;&amp;lt;cruising&amp;gt;&amp;lt;vacations...</td>\n",
       "      <td>What are some Caribbean cruises for October?</td>\n",
       "      <td>467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>&amp;lt;p&amp;gt;This was one of our definition questi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:22:33.760</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-08-26T00:04:13.520</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>&amp;lt;guides&amp;gt;&amp;lt;extreme-tourism&amp;gt;&amp;lt;amazo...</td>\n",
       "      <td>How can I find a guide that will take me safel...</td>\n",
       "      <td>2252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&amp;lt;p&amp;gt;One way would be to go through an Adv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:24:28.080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-06-21T20:24:28.080</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>&amp;lt;p&amp;gt;Singapore Airlines has an all-busines...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:24:57.160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-09T09:55:22.743</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>&amp;lt;loyalty-programs&amp;gt;&amp;lt;routes&amp;gt;&amp;lt;ewr&amp;...</td>\n",
       "      <td>Does Singapore Airlines offer any reward seats...</td>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>770</td>\n",
       "      <td>5</td>\n",
       "      <td>&amp;lt;p&amp;gt;Another definition question that inte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:25:56.787</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-10-12T20:49:08.110</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>&amp;lt;romania&amp;gt;&amp;lt;transportation&amp;gt;</td>\n",
       "      <td>What is the easiest transportation to use thro...</td>\n",
       "      <td>432.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AcceptedAnswerId AnswerCount  \\\n",
       "Id                                \n",
       "1               393           4   \n",
       "2               NaN           8   \n",
       "3               NaN         NaN   \n",
       "4               NaN           1   \n",
       "5               770           5   \n",
       "\n",
       "                                                 Body  \\\n",
       "Id                                                      \n",
       "1   &lt;p&gt;My fiancée and I are looking for a go...   \n",
       "2   &lt;p&gt;This was one of our definition questi...   \n",
       "3   &lt;p&gt;One way would be to go through an Adv...   \n",
       "4   &lt;p&gt;Singapore Airlines has an all-busines...   \n",
       "5   &lt;p&gt;Another definition question that inte...   \n",
       "\n",
       "                 ClosedDate CommentCount CommunityOwnedDate  \\\n",
       "Id                                                            \n",
       "1   2013-02-25T23:52:47.953            4                NaN   \n",
       "2                       NaN            4                NaN   \n",
       "3                       NaN            2                NaN   \n",
       "4                       NaN            1                NaN   \n",
       "5                       NaN            0                NaN   \n",
       "\n",
       "               CreationDate FavoriteCount  Id         LastActivityDate  ...  \\\n",
       "Id                                                                      ...   \n",
       "1   2011-06-21T20:19:34.730           NaN   1  2012-05-24T14:52:14.760  ...   \n",
       "2   2011-06-21T20:22:33.760             5   2  2018-08-26T00:04:13.520  ...   \n",
       "3   2011-06-21T20:24:28.080           NaN   3  2011-06-21T20:24:28.080  ...   \n",
       "4   2011-06-21T20:24:57.160           NaN   4  2013-01-09T09:55:22.743  ...   \n",
       "5   2011-06-21T20:25:56.787             2   5  2012-10-12T20:49:08.110  ...   \n",
       "\n",
       "   LastEditorDisplayName LastEditorUserId OwnerDisplayName OwnerUserId  \\\n",
       "Id                                                                       \n",
       "1                    NaN              101              NaN           9   \n",
       "2                    NaN            51577              NaN          13   \n",
       "3                    NaN              NaN              NaN           9   \n",
       "4                    NaN              693              NaN          24   \n",
       "5                    NaN              101              NaN          13   \n",
       "\n",
       "   ParentId PostTypeId  Score  \\\n",
       "Id                              \n",
       "1       NaN          1      8   \n",
       "2       NaN          1     38   \n",
       "3         2          2     15   \n",
       "4       NaN          1      8   \n",
       "5       NaN          1     14   \n",
       "\n",
       "                                                 Tags  \\\n",
       "Id                                                      \n",
       "1   &lt;caribbean&gt;&lt;cruising&gt;&lt;vacations...   \n",
       "2   &lt;guides&gt;&lt;extreme-tourism&gt;&lt;amazo...   \n",
       "3                                                       \n",
       "4   &lt;loyalty-programs&gt;&lt;routes&gt;&lt;ewr&...   \n",
       "5               &lt;romania&gt;&lt;transportation&gt;   \n",
       "\n",
       "                                                Title ViewCount  \n",
       "Id                                                               \n",
       "1        What are some Caribbean cruises for October?     467.0  \n",
       "2   How can I find a guide that will take me safel...    2252.0  \n",
       "3                                                           NaN  \n",
       "4   Does Singapore Airlines offer any reward seats...     266.0  \n",
       "5   What is the easiest transportation to use thro...     432.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(rows)    \n",
    "df = df.set_index('Id', drop=False)\n",
    "df['Title'] = df['Title'].fillna('').astype('str')\n",
    "df['Tags'] = df['Tags'].fillna('').astype('str')\n",
    "df['Body'] = df['Body'].fillna('').astype('str')\n",
    "df['Id'] = df['Id'].astype('int')\n",
    "df['PostTypeId'] = df['PostTypeId'].astype('int')\n",
    "df['ViewCount'] = df['ViewCount'].astype('float')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do I need a US visa to transit (or layover) through an American airport?',\n",
       " 'How much electronics and other valuables can I bring duty-free when going to India?',\n",
       " 'How to get from Nice to Monaco by public transport?',\n",
       " 'Should my first trip be to the country which issued my Schengen Visa?',\n",
       " 'Can I cross the USA-Canada border with a birth certificate and a passport locator number?',\n",
       " \"What's the difference between 'Redress Number' and 'Known Traveler Number'? Do I need both for TSA PreCheck?\",\n",
       " 'Can I use Google Maps traffic information to estimate driving time for a specific date/time?',\n",
       " 'Is there a way to find out if I need a transit visa for a layover in the UK?',\n",
       " 'Are aerosol cans allowed and safe, in checked luggage?',\n",
       " 'How to track my UK Visa Application Status?',\n",
       " \"When applying for an Indian Passport, how do I know if I'm in the ECR or non-ECR category?\",\n",
       " 'Are battery packs allowed in hand luggage?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df[df['ViewCount'] > 250000]['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Germany Work visa dates']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df[df['ViewCount'] < 5]['Title']) # questions nobody was interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use **tokenizer** to convert words to digits\n",
    "\n",
    "Tokenizer converts the first N different words of a given text that it finds into digits.\n",
    "Each word is represented with a unique number. This can also be done with vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id\n",
       "1    &lt;p&gt;My fiancée and I are looking for a go...\n",
       "2    &lt;p&gt;This was one of our definition questi...\n",
       "3    &lt;p&gt;One way would be to go through an Adv...\n",
       "4    &lt;p&gt;Singapore Airlines has an all-busines...\n",
       "5    &lt;p&gt;Another definition question that inte...\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['Body'] + df['Title']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df['Body'] + df['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 19523634\n"
     ]
    }
   ],
   "source": [
    "# Compute TF/IDF Values\n",
    "\n",
    "total_count = sum(tokenizer.word_counts.values())\n",
    "print(\"Total count: {}\".format(total_count))\n",
    "idf = { k: np.log(total_count/v) for (k,v) in tokenizer.word_counts.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download pre-trained word2vec embeddings\n",
    "\n",
    "import gensim\n",
    "\n",
    "glove_100d = utils.get_file(\n",
    "    fname='glove.6B.100d.txt',\n",
    "    origin='https://storage.googleapis.com/deep-learning-cookbook/glove.6B.100d.txt',\n",
    ")\n",
    "\n",
    "w2v_100d = glove_100d + '.w2v'\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_100d, w2v_100d)\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(w2v_100d)\n",
    "\n",
    "w2v_weights = np.zeros((VOCAB_SIZE, w2v_model.syn0.shape[1]))\n",
    "idf_weights = np.zeros((VOCAB_SIZE, 1))\n",
    "\n",
    "for k, v in tokenizer.word_index.items():\n",
    "    if v >= VOCAB_SIZE:\n",
    "        continue\n",
    "    \n",
    "    if k in w2v_model:\n",
    "        w2v_weights[v] = w2v_model[k]\n",
    "    \n",
    "    idf_weights[v] = idf[k]\n",
    "    \n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_tokens'] = tokenizer.texts_to_sequences(df['Title'])\n",
    "df['body_tokens'] = tokenizer.texts_to_sequences(df['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>Id</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>...</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Title</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>body_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>393</td>\n",
       "      <td>4</td>\n",
       "      <td>&amp;lt;p&amp;gt;My fiancée and I are looking for a go...</td>\n",
       "      <td>2013-02-25T23:52:47.953</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:19:34.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-05-24T14:52:14.760</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>&amp;lt;caribbean&amp;gt;&amp;lt;cruising&amp;gt;&amp;lt;vacations...</td>\n",
       "      <td>What are some Caribbean cruises for October?</td>\n",
       "      <td>467.0</td>\n",
       "      <td>[68, 20, 65, 2349, 3033, 15, 1194]</td>\n",
       "      <td>[2, 4, 1, 37, 9575, 9, 12, 20, 404, 15, 6, 175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>&amp;lt;p&amp;gt;This was one of our definition questi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:22:33.760</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-08-26T00:04:13.520</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>&amp;lt;guides&amp;gt;&amp;lt;extreme-tourism&amp;gt;&amp;lt;amazo...</td>\n",
       "      <td>How can I find a guide that will take me safel...</td>\n",
       "      <td>2252.0</td>\n",
       "      <td>[91, 33, 12, 125, 6, 708, 16, 35, 106, 93, 268...</td>\n",
       "      <td>[2, 4, 1, 32, 59, 56, 13, 330, 2583, 118, 34, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AcceptedAnswerId AnswerCount  \\\n",
       "Id                                \n",
       "1               393           4   \n",
       "2               NaN           8   \n",
       "\n",
       "                                                 Body  \\\n",
       "Id                                                      \n",
       "1   &lt;p&gt;My fiancée and I are looking for a go...   \n",
       "2   &lt;p&gt;This was one of our definition questi...   \n",
       "\n",
       "                 ClosedDate CommentCount CommunityOwnedDate  \\\n",
       "Id                                                            \n",
       "1   2013-02-25T23:52:47.953            4                NaN   \n",
       "2                       NaN            4                NaN   \n",
       "\n",
       "               CreationDate FavoriteCount  Id         LastActivityDate  ...  \\\n",
       "Id                                                                      ...   \n",
       "1   2011-06-21T20:19:34.730           NaN   1  2012-05-24T14:52:14.760  ...   \n",
       "2   2011-06-21T20:22:33.760             5   2  2018-08-26T00:04:13.520  ...   \n",
       "\n",
       "   OwnerDisplayName OwnerUserId ParentId PostTypeId Score  \\\n",
       "Id                                                          \n",
       "1               NaN           9      NaN          1     8   \n",
       "2               NaN          13      NaN          1    38   \n",
       "\n",
       "                                                 Tags  \\\n",
       "Id                                                      \n",
       "1   &lt;caribbean&gt;&lt;cruising&gt;&lt;vacations...   \n",
       "2   &lt;guides&gt;&lt;extreme-tourism&gt;&lt;amazo...   \n",
       "\n",
       "                                                Title ViewCount  \\\n",
       "Id                                                                \n",
       "1        What are some Caribbean cruises for October?     467.0   \n",
       "2   How can I find a guide that will take me safel...    2252.0   \n",
       "\n",
       "                                         title_tokens  \\\n",
       "Id                                                      \n",
       "1                  [68, 20, 65, 2349, 3033, 15, 1194]   \n",
       "2   [91, 33, 12, 125, 6, 708, 16, 35, 106, 93, 268...   \n",
       "\n",
       "                                          body_tokens  \n",
       "Id                                                     \n",
       "1   [2, 4, 1, 37, 9575, 9, 12, 20, 404, 15, 6, 175...  \n",
       "2   [2, 4, 1, 32, 59, 56, 13, 330, 2583, 118, 34, ...  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can train a classifier that can predict if a answer matches a given question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'title': array([[   101,     33,     12,    125, 167588,  13271,     15,    601],\n",
       "         [   101,     33,     12,    125, 167588,  13271,     15,    601],\n",
       "         [   101,     33,     12,    125, 167588,  13271,     15,    601]],\n",
       "        dtype=int32),\n",
       "  'body': array([[     2,      4,      1,    384,     67,     11,    199,      6,\n",
       "            1665,     18,   4185,      5,    225,   1959,     11,      7,\n",
       "             773,      6,   1665,     15,      5,    187,    101,     11,\n",
       "             177,    598,      2,      4,      1,      3,      3,      2,\n",
       "               4,      1,      2,    242,    244,      8,     46,     12,\n",
       "             161,    163,     44, 167587,    452,      8,    250,      8,\n",
       "             126,    392,    424,    122,      8,      1,      2,      4,\n",
       "               1,      3,      3,      2,      4,      1,     34,    101,\n",
       "              33,     56,     87,    132,    147,   1684,     18,      5,\n",
       "            4185,    225,    456,   1766,      7,    125,     96,      5,\n",
       "           13271,     15,     65,    601,     12,     80,    416,      7,\n",
       "             124,     30,    186,     30,      7,    567,      5,   2965,\n",
       "               2,      4,      1,      3],\n",
       "         [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      2,      4,      1,     91,     33,     12,\n",
       "             125,      6,    331,      7,    544,     93,    123,     15,\n",
       "               6,    403,     10,  19218,     25,  24159,   2199,     13,\n",
       "            1174,   1477,      2,      4,      1,      3,      3,      2,\n",
       "               4,      1,     49,     11,    117,     23,     39,     14,\n",
       "               6,    371,    653,   1948,    146,     22,    160,    454,\n",
       "              15,    254,      6,    371,     16,     33,    125,      6,\n",
       "             331,     15,     93,      9,     87,    144,    211,     41,\n",
       "              54,    106,    775,     13,     93,    470,    417,    245,\n",
       "               2,      4,      1,      3],\n",
       "         [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      2,      4,      1,     12,     22,   1224,\n",
       "             692,     10,    202,      9,      5,     60,     91,    149,\n",
       "              54,     12,     19,    283,      7,    123,     10,      5,\n",
       "             295,    258,    262,    198,    230,     80,     12,    210,\n",
       "               7,    104,      5,    129,    230,  17820,      9,     23,\n",
       "              50,     68,   2088,     20,    276,      7,    199,      5,\n",
       "             228,      9,    557,    126,    134,      6,    196,    102,\n",
       "               2,      4,      1,      3]], dtype=int32)},\n",
       " array([1, 0, 0]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# We can create a data generator that will randomly title and body tokens for questions.  We'll use random text\n",
    "# from other questions as a negative example when necessary.\n",
    "def data_generator(batch_size, negative_samples=1):\n",
    "    questions = df[df['PostTypeId'] == 1]\n",
    "    all_q_ids = list(questions.index)\n",
    "        \n",
    "    batch_x_a = []\n",
    "    batch_x_b = []\n",
    "    batch_y = []\n",
    "    \n",
    "    def _add(x_a, x_b, y):\n",
    "        batch_x_a.append(x_a[:MAX_DOC_LEN])\n",
    "        batch_x_b.append(x_b[:MAX_DOC_LEN])\n",
    "        batch_y.append(y)\n",
    "    \n",
    "    while True:\n",
    "        questions = questions.sample(frac=1.0)\n",
    "        \n",
    "        for i, q in questions.iterrows():\n",
    "            _add(q['title_tokens'], q['body_tokens'], 1)\n",
    "            \n",
    "            negative_q = random.sample(all_q_ids, negative_samples)\n",
    "            for nq_id in negative_q:\n",
    "                _add(q['title_tokens'], df.at[nq_id, 'body_tokens'], 0)            \n",
    "            \n",
    "            if len(batch_y) >= batch_size:\n",
    "                yield ({\n",
    "                    'title': pad_sequences(batch_x_a, maxlen=None),\n",
    "                    'body': pad_sequences(batch_x_b, maxlen=None),\n",
    "                }, np.asarray(batch_y))\n",
    "                \n",
    "                batch_x_a = []\n",
    "                batch_x_b = []\n",
    "                batch_y = []\n",
    "\n",
    "dg = data_generator(1, 2)\n",
    "next(dg)\n",
    "#next(dg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Lookups\n",
    "\n",
    "Let's define a helper class for looking up our embedding results.  We'll use it\n",
    "to verify our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df[df['PostTypeId'] == 1]['Title'].reset_index(drop=True)\n",
    "question_tokens = pad_sequences(tokenizer.texts_to_sequences(questions))\n",
    "\n",
    "class EmbeddingWrapper(object):\n",
    "    def __init__(self, model):\n",
    "        self._r = questions\n",
    "        self._i = {i:s for (i, s) in enumerate(questions)}\n",
    "        self._w = model.predict({'title': question_tokens}, verbose=1, batch_size=1024)\n",
    "        self._model = model\n",
    "        self._norm = np.sqrt(np.sum(self._w * self._w + 1e-5, axis=1))\n",
    "\n",
    "    def nearest(self, sentence, n=10):\n",
    "        x = tokenizer.texts_to_sequences([sentence])\n",
    "        if len(x[0]) < MIN_DOC_LEN:\n",
    "            x[0] += [0] * (MIN_DOC_LEN - len(x))\n",
    "        e = self._model.predict(np.asarray(x))[0]\n",
    "        norm_e = np.sqrt(np.dot(e, e))\n",
    "        dist = np.dot(self._w, e) / (norm_e * self._norm)\n",
    "\n",
    "        top_idx = np.argsort(dist)[-n:]\n",
    "        return pd.DataFrame.from_records([\n",
    "            {'question': self._r[i], 'dist': float(dist[i])}\n",
    "            for i in top_idx\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first model will just sum up the embeddings of each token.\n",
    "# The similarity between documents will be the dot product of the final embedding.\n",
    "# The dot product of normalized vectors is nothing else then the Cosine-Similarity (do the vectors point in same direction?)\n",
    "# https://de.wikipedia.org/wiki/Kosinus-%C3%84hnlichkeit\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def sum_model(embedding_size, vocab_size, embedding_weights=None, idf_weights=None):\n",
    "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
    "\n",
    "    def make_embedding(name):\n",
    "        if embedding_weights is not None:\n",
    "            embedding = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=w2v_weights.shape[1], \n",
    "                                         weights=[w2v_weights], trainable=False, \n",
    "                                         name='%s/embedding' % name)\n",
    "        else:\n",
    "            embedding = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=embedding_size,\n",
    "                                        name='%s/embedding' % name)\n",
    "\n",
    "        if idf_weights is not None:\n",
    "            idf = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=1, \n",
    "                                   weights=[idf_weights], trainable=False,\n",
    "                                   name='%s/idf' % name)\n",
    "        else:\n",
    "            idf = layers.Embedding(mask_zero=True, input_dim=vocab_size, output_dim=1,\n",
    "                                   name='%s/idf' % name)\n",
    "            \n",
    "        return embedding, idf\n",
    "    \n",
    "    embedding_a, idf_a = make_embedding('a')\n",
    "    embedding_b, idf_b = embedding_a, idf_a\n",
    "#     embedding_b, idf_b = make_embedding('b')\n",
    "\n",
    "    mask = layers.Masking(mask_value=0)\n",
    "    def _combine_and_sum(args):\n",
    "        [embedding, idf] = args\n",
    "        return K.sum(embedding * K.abs(idf), axis=1)\n",
    "\n",
    "    sum_layer = layers.Lambda(_combine_and_sum, name='combine_and_sum')\n",
    "\n",
    "    sum_a = sum_layer([mask(embedding_a(title)), idf_a(title)])\n",
    "    sum_b = sum_layer([mask(embedding_b(body)), idf_b(body)])\n",
    "\n",
    "    sim = layers.dot([sum_a, sum_b], axes=1, normalize=True)\n",
    "    sim_model = models.Model(\n",
    "        inputs=[title, body],\n",
    "        outputs=[sim],\n",
    "    )\n",
    "    sim_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    sim_model.summary()\n",
    "\n",
    "    embedding_model = models.Model(\n",
    "        inputs=[title],\n",
    "        outputs=[sum_a]\n",
    "    )\n",
    "    return sim_model, embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try using our model with pretrained weights from word2vec\n",
    "\n",
    "sum_model_precomputed, sum_embedding_precomputed = sum_model(\n",
    "    embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE,\n",
    "    embedding_weights=w2v_weights, idf_weights=idf_weights\n",
    ")\n",
    "\n",
    "x, y = next(data_generator(batch_size=4096))\n",
    "sum_model_precomputed.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_QUESTIONS = [\n",
    "    'Roundtrip ticket versus one way',\n",
    "    'Shinkansen from Kyoto to Hiroshima',\n",
    "    'Bus tour of Germany',\n",
    "]\n",
    "\n",
    "def evaluate_sample(lookup):\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    results = []\n",
    "    for q in SAMPLE_QUESTIONS:\n",
    "        print(q)\n",
    "        q_res = lookup.nearest(q, n=4)\n",
    "        q_res['result'] = q_res['question']\n",
    "        q_res['question'] = q\n",
    "        results.append(q_res)\n",
    "\n",
    "    return pd.concat(results)\n",
    "\n",
    "lookup = EmbeddingWrapper(model=sum_embedding_precomputed)\n",
    "evaluate_sample(lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own network\n",
    "\n",
    "The results are okay but not great... instead of using the word2vec embeddings, what happens if we train our network end-to-end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 13:52:55.718056 140355969009472 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0809 13:52:55.749694 140355969009472 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0809 13:52:55.750634 140355969009472 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0809 13:52:55.817876 140355969009472 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0809 13:52:55.838687 140355969009472 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0809 13:52:55.843587 140355969009472 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "body (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "a/embedding (Embedding)         (None, None, 100)    25000000    title[0][0]                      \n",
      "                                                                 body[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 100)    0           a/embedding[0][0]                \n",
      "                                                                 a/embedding[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "a/idf (Embedding)               (None, None, 1)      250000      title[0][0]                      \n",
      "                                                                 body[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)        (None, 100)          0           masking_1[0][0]                  \n",
      "                                                                 a/idf[0][0]                      \n",
      "                                                                 masking_1[1][0]                  \n",
      "                                                                 a/idf[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           combine_and_sum[0][0]            \n",
      "                                                                 combine_and_sum[1][0]            \n",
      "==================================================================================================\n",
      "Total params: 25,250,000\n",
      "Trainable params: 25,250,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 13:52:56.223479 140355969009472 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 210s 210ms/step - loss: 0.2667 - acc: 0.9176\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 208s 208ms/step - loss: 0.1547 - acc: 0.9716\n",
      "Epoch 3/10\n",
      " 408/1000 [===========>..................] - ETA: 2:03 - loss: 0.1333 - acc: 0.9791"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2e4e19d89725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sum_model_trained, sum_embedding_trained = sum_model(\n",
    "    embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE, \n",
    "    embedding_weights=None,\n",
    "    idf_weights=None\n",
    ")\n",
    "sum_model_trained.fit_generator(\n",
    "    data_generator(batch_size=128),\n",
    "    epochs=10,\n",
    "    steps_per_epoch=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup = EmbeddingWrapper(model=sum_embedding_trained)\n",
    "evaluate_sample(lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "Using a sum-of-embeddings model works well. What happens if we try to make a simple CNN model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(embedding_size, vocab_size):\n",
    "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
    "\n",
    "    embedding = layers.Embedding(\n",
    "        mask_zero=False,\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_size,\n",
    "    )\n",
    "\n",
    "\n",
    "    def _combine_sum(v):\n",
    "        return K.sum(v, axis=1)\n",
    "\n",
    "    cnn_1 = layers.Convolution1D(256, 3)\n",
    "    cnn_2 = layers.Convolution1D(256, 3)\n",
    "    cnn_3 = layers.Convolution1D(256, 3)\n",
    "    \n",
    "    global_pool = layers.GlobalMaxPooling1D()\n",
    "    local_pool = layers.MaxPooling1D(strides=2, pool_size=3)\n",
    "\n",
    "    def forward(input):\n",
    "        embed = embedding(input)\n",
    "        return global_pool(\n",
    "            cnn_2(local_pool(cnn_1(embed))))\n",
    "\n",
    "    sum_a = forward(title)\n",
    "    sum_b = forward(body)\n",
    "\n",
    "    sim = layers.dot([sum_a, sum_b], axes=1, normalize=False)\n",
    "    sim_model = models.Model(\n",
    "        inputs=[title, body],\n",
    "        outputs=[sim],\n",
    "    )\n",
    "    sim_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    embedding_model = models.Model(\n",
    "        inputs=[title],\n",
    "        outputs=[sum_a]\n",
    "    )\n",
    "    return sim_model, embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn, cnn_embedding = cnn_model(embedding_size=25, vocab_size=VOCAB_SIZE)\n",
    "cnn.summary()\n",
    "cnn.fit_generator(\n",
    "    data_generator(batch_size=128),\n",
    "    epochs=10,\n",
    "    steps_per_epoch=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup = EmbeddingWrapper(model=cnn_embedding)\n",
    "evaluate_sample(lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "\n",
    "We can also make an LSTM model.  Warning, this will be very slow to train and evaluate unless you have a relatively fast GPU to run it on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model(embedding_size, vocab_size):\n",
    "    title = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    body = layers.Input(shape=(None,), dtype='int32', name='body')\n",
    "\n",
    "    embedding = layers.Embedding(\n",
    "        mask_zero=True,\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_size,\n",
    "#         weights=[w2v_weights],\n",
    "#         trainable=False\n",
    "    )\n",
    "\n",
    "    lstm_1 = layers.LSTM(units=512, return_sequences=True)\n",
    "    lstm_2 = layers.LSTM(units=512, return_sequences=False)\n",
    "    \n",
    "    sum_a = lstm_2(lstm_1(embedding(title)))\n",
    "    sum_b = lstm_2(lstm_1(embedding(body)))\n",
    "\n",
    "    sim = layers.dot([sum_a, sum_b], axes=1, normalize=True)\n",
    "#     sim = layers.Activation(activation='sigmoid')(sim)\n",
    "    sim_model = models.Model(\n",
    "        inputs=[title, body],\n",
    "        outputs=[sim],\n",
    "    )\n",
    "    sim_model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    embedding_model = models.Model(\n",
    "        inputs=[title],\n",
    "        outputs=[sum_a]\n",
    "    )\n",
    "    return sim_model, embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm, lstm_embedding = lstm_model(embedding_size=EMBEDDING_SIZE, vocab_size=VOCAB_SIZE)\n",
    "lstm.summary()\n",
    "lstm.fit_generator(\n",
    "    data_generator(batch_size=128),\n",
    "    epochs=10,\n",
    "    steps_per_epoch=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup = EmbeddingWrapper(model=lstm_embedding)\n",
    "evaluate_sample(lookup)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
